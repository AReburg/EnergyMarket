{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules/libraries\n",
    "import warnings \n",
    "warnings.simplefilter(action='ignore')\n",
    "from entsoe import EntsoePandasClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pandas import json_normalize\n",
    "from bs4 import BeautifulSoup\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import geojson\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "# geo libraries\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "import chardet\n",
    "from scipy import spatial\n",
    "from scipy.spatial import KDTree\n",
    "from shapely import wkt\n",
    "from geopy.distance import geodesic\n",
    "from geopandas import GeoDataFrame\n",
    "import shapely.ops\n",
    "import shapely.geometry\n",
    "from shapely.geometry import Polygon, LineString\n",
    "cwd = Path().resolve()\n",
    "\n",
    "import sqlite3\n",
    "# visualisation\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl \n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "s = requests.Session()\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#------\n",
    "header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "   'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "   'Accept-Encoding': 'none',\n",
    "   'Accept-Language': 'en-US,en;q=0.8',\n",
    "   'Connection': 'keep-alive'}\n",
    "##--\n",
    "\n",
    "class InvalidCoordinate(Exception):\n",
    "    pass\n",
    "\n",
    "# conn = sqlite3.connect('source.db')\n",
    "# vessel = pd.read_sql_query(\"SELECT imo FROM vessel_information\", conn) #= pd.read_csv('./data/lng_tankers_lat.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decd4b5",
   "metadata": {},
   "source": [
    "### 1. Scrape VesselFinder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceac07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SQLITE: DELETE FROM lng WHERE date = '2022-12-28'\n",
    "\n",
    "interesting news shipyard: https://www.ship-technology.com/news/daewoo-shipbuilding-contract-lng-ships/\n",
    "\n",
    "\"\"\"\n",
    "def get_current_vessel_location_V2(imo, delay=0.5):\n",
    "    try:\n",
    "        url = f\"https://www.vesselfinder.com/en/vessels/VOS-TRAVELLER-IMO-{imo}\"\n",
    "        r = s.get(url, headers=header)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        data = json.loads(soup.find(\"div\", {\"id\": \"djson\"})['data-json'])\n",
    "        \n",
    "        #  get latitude and longitude\n",
    "        try:\n",
    "            lat = data['ship_lat']\n",
    "            lon = data['ship_lon']\n",
    "        except:\n",
    "            lat = np.nan\n",
    "            lon = np.nan\n",
    "    \n",
    "        # get from and to port \n",
    "        try:\n",
    "            _from = soup.find_all(\"a\", {\"class\": \"_npNa\"})[0]['href']\n",
    "            _from_text = soup.find_all(\"a\", {\"class\": \"_npNa\"})[0].text\n",
    "        except:\n",
    "            _from = np.nan\n",
    "            _from_text = np.nan\n",
    "    \n",
    "        try:\n",
    "            _to = soup.find_all(\"a\", {\"class\": \"_npNa\"})[1]['href']\n",
    "            _to_text = soup.find_all(\"a\", {\"class\": \"_npNa\"})[1].text\n",
    "        except:\n",
    "            _to = np.nan\n",
    "            _to_text = np.nan\n",
    "\n",
    "        try:\n",
    "            ship_cog = data['ship_cog']\n",
    "            ship_sog = data['ship_sog']\n",
    "            ship_type = data['ship_type']\n",
    "            no_pc = data['no_pc']\n",
    "            mmsi = int(data['mmsi'])\n",
    "        except:\n",
    "            ship_cog = np.nan\n",
    "            ship_sog = np.nan\n",
    "            ship_type = np.nan\n",
    "            no_pc = np.nan\n",
    "            mmsi = np.nan\n",
    "        \n",
    "        # mining of all the dates\n",
    "        today = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "        \n",
    "        try:\n",
    "            received = soup.find(\"td\", {\"class\": \"v3 ttt1 valm0\"})['data-title']\n",
    "            date_object = received.upper()\n",
    "            date_object = datetime.strptime(date_object, \"%b %d, %Y %H:%M UTC\")\n",
    "            date_object = date_object.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            # print(date_object)\n",
    "        except (TypeError, ValueError):\n",
    "            date_object = np.nan\n",
    "    \n",
    "        try:\n",
    "            eta_raw = soup.find(\"span\", {\"class\": \"_mcol12\"}).text\n",
    "            try:\n",
    "                eta = eta_raw.split(\"ETA: \")[1].upper()\n",
    "                # print(eta)\n",
    "            except IndexError:\n",
    "                eta = eta_raw.split(\"ATA: \")[1].upper()\n",
    "            except:\n",
    "                raise IndexError\n",
    "        except AttributeError:\n",
    "            # print(imo, \"AtributesError\", soup.find(\"span\", {\"class\": \"_mcol12\"}))\n",
    "            eta = np.nan\n",
    "        except IndexError:\n",
    "            eta = np.nan\n",
    "\n",
    "        try:\n",
    "            atd_raw = soup.find_all(\"div\", {\"class\": \"_value\"})[1].text\n",
    "        except (AttributeError, IndexError):\n",
    "            atd_raw = np.nan\n",
    "\n",
    "        # The latitude must be a number between -90 and 90 and the longitude between -180 and 180.\n",
    "        if (lat < -90 or lat > 90) or (lon < -180 or lon > 180):\n",
    "            raise InvalidCoordinate\n",
    "        \n",
    "        return pd.Series([lat, lon, _from, _to, date_object, eta, atd_raw, ship_cog, ship_sog, ship_type, mmsi, today])\n",
    "    \n",
    "    except InvalidCoordinate:\n",
    "        print(f\"Invalid Coordinate of vessel: {imo}\")\n",
    "        return pd.Series([lat, lon, _from, _to, date_object, eta, atd_raw, ship_cog, ship_sog, ship_type, mmsi, today])\n",
    "    \n",
    "    except Exception as e:\n",
    "        input(f\"Change vpn: {imo}\")\n",
    "        return pd.Series([lat, lon, _from, _to, date_object, eta, atd_raw, ship_cog, ship_sog, ship_type, mmsi, today])\n",
    "\n",
    "\n",
    "def run():\n",
    "    # print(f\"start length: {vessel.shape[0]}\")\n",
    "    t0 = time.time()\n",
    "    conn = sqlite3.connect('source.db')\n",
    "    df = pd.read_sql_query(\"SELECT imo FROM vessel_information\", conn)\n",
    "    df.drop_duplicates(subset=\"imo\", keep='first', inplace=True)\n",
    "    df.dropna(subset=['imo'], inplace=True)\n",
    "    # df = df.iloc[550:650,:]\n",
    "    df2 = df.copy()\n",
    "    df2[['lat', 'lon', 'fromPort', 'toPort', 'received', 'eta', 'atd',\n",
    "         'ship_cog', 'ship_sog', 'ship_type', 'mmsi','scrape_date']] = df.apply(lambda x: get_current_vessel_location_V2(x['imo'], delay=0.1), axis=1)\n",
    "    df2['mmsi'] = df2['mmsi'].fillna(0).astype('int64')\n",
    "    print(f\"Processing took: {round(time.time()-t0)} s\") # ~6.4 min\n",
    "    return df2\n",
    "\n",
    "df = run()\n",
    "\n",
    "#cwd = Path(__file__).parent\n",
    "dirname = cwd.parent / \"db\"\n",
    "db_name = 'file_both.db'\n",
    "con = sqlite3.connect(db_name) #os.path.join(dirname, db_name)\n",
    "cur = con.cursor()\n",
    "create_table = (f'''CREATE TABLE IF NOT EXISTS VesselLocation (imo INT, lat TEXT, lon TEXT, fromPort TEXT, \n",
    "toPort TEXT, received TEXT, eta TEXT, atd TEXT, ship_cog TEXT, ship_sog TEXT, ship_type TEXT, no_pc TEXT,\n",
    "mmsi, scrape_date TEXT)''')\n",
    "cur.execute(create_table)\n",
    "\n",
    "#create_table = (f'''CREATE TABLE IF NOT EXISTS VesselLocation (imo TEXT)''') #(f\"\"\"CREATE TABLE IF NOT EXISTS VesselLocation (rowid INTEGER PRIMARY KEY) WITHOUT ROWID;\"\"\")#\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def write_to_db(con, cur, params):\n",
    "    con.commit()\n",
    "    \n",
    "    \"\"\" dynamic addition of columns \n",
    "    columns = [i[1] for i in cur.execute('PRAGMA table_info(VesselLocation)')]\n",
    "    for idx,i in enumerate(params['columns']):\n",
    "        if i not in columns:\n",
    "            #print(i)\n",
    "            cur.execute(f'ALTER TABLE VesselLocation ADD COLUMN {i} TEXT')\n",
    "            con.commit()\n",
    "    \"\"\"\n",
    "    query_sign = ''.join(['?' if i == len(params['columns'])-1 else '?, ' for i in range(len(params['columns']))])\n",
    "    cols = ''.join([i if i == params['columns'][-1] else i+', ' for i in params['columns']])\n",
    "    sqlite_insert_with_param1 = f\"\"\"INSERT INTO VesselLocation  ({cols}) VALUES ({query_sign});\"\"\"\n",
    "    data_tuple = tuple(params['values'])\n",
    "    cur.execute(sqlite_insert_with_param1, data_tuple)\n",
    "    con.commit()\n",
    "\n",
    "\n",
    "title = df.columns.tolist()\n",
    "for index, row in df.iterrows():\n",
    "    results = OrderedDict()\n",
    "    value = row.values.tolist()\n",
    "    results['columns'] = title\n",
    "    results['values'] = value\n",
    "    write_to_db(con, cur, results)\n",
    "    \n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03d313",
   "metadata": {},
   "source": [
    "### 2. Scrape MarineTraffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"accept-encoding\": \"gzip, deflate\",\n",
    "    \"user-agent\": \"Mozilla/5.0\",\n",
    "    \"x-requested-with\": \"XMLHttpRequest\"\n",
    "}\n",
    "    \n",
    "def get_ship_position(shipid, delay=0.5):\n",
    "    \"\"\"source: https://stackoverflow.com/questions/68654987/scraping-data-from-marinetraffic-page\"\"\"\n",
    "    \n",
    "    url = f\"https://www.marinetraffic.com/vesselDetails/latestPosition/{shipid}\"\n",
    "    response = session.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    time.sleep(delay)\n",
    "    data = response.json()\n",
    "    try:\n",
    "        lat = data[\"lat\"]\n",
    "        lon = data[\"lon\"]\n",
    "    except:\n",
    "        lat = np.nan\n",
    "        lon = np.nan\n",
    "    try:\n",
    "        ts = datetime.utcfromtimestamp(data[\"lastPos\"])\n",
    "        date_object = ts.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    except TypeError:\n",
    "        ts = None\n",
    "        date_object = np.nan\n",
    "        \n",
    "    # mining of all the dates\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    print(\"{}, {} Last known position: {} / {} @ {}\".format(today, shipid, lat, lon, ts))\n",
    "    return pd.Series([lat, lon, date_object, today])\n",
    "\n",
    "\n",
    "def scrape_data():\n",
    "    # print(f\"start length: {vessel.shape[0]}\")\n",
    "    t0 = time.time()\n",
    "    conn = sqlite3.connect('source.db')\n",
    "    df = pd.read_sql_query(\"SELECT imo, shipid FROM marinetraffic_vessel_information\", conn)\n",
    "    df.drop_duplicates(subset=\"imo\", keep='first', inplace=True)\n",
    "    df.dropna(subset=['imo'], inplace=True)\n",
    "    # df = df.iloc[70:100,:]\n",
    "    df2 = df.copy()\n",
    "    df2[['lat', 'lon', 'received','scrape_date']] = df.apply(lambda x: get_ship_position(x['shipid'], delay=0.1), axis=1)\n",
    "    print(f\"Processing took: {round(time.time()-t0)} s\")\n",
    "    return df2\n",
    "\n",
    "df = scrape_data()\n",
    "df.head()\n",
    "\n",
    "def marinetraffic_write_to_db(con, cur, params):\n",
    "    con.commit()\n",
    "    query_sign = ''.join(['?' if i == len(params['columns'])-1 else '?, ' for i in range(len(params['columns']))])\n",
    "    cols = ''.join([i if i == params['columns'][-1] else i+', ' for i in params['columns']])\n",
    "    sqlite_insert_with_param1 = f\"\"\"INSERT INTO MarineTrafficVesselLocation ({cols}) VALUES ({query_sign});\"\"\"\n",
    "    data_tuple = tuple(params['values'])\n",
    "    cur.execute(sqlite_insert_with_param1, data_tuple)\n",
    "    con.commit()\n",
    "\n",
    "    \n",
    "def saving(df):\n",
    "    dirname = cwd.parent / \"db\"\n",
    "    db_name = 'file_both.db'\n",
    "    con = sqlite3.connect(db_name)\n",
    "    cur = con.cursor()\n",
    "    create_table = (f'''CREATE TABLE IF NOT EXISTS MarineTrafficVesselLocation (imo INT, shipid TEXT, lat TEXT, lon TEXT,\n",
    "    received TEXT, scrape_date DATE)''')\n",
    "    cur.execute(create_table)\n",
    "\n",
    "    title = df.columns.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        results = OrderedDict()\n",
    "        value = row.values.tolist()\n",
    "        results['columns'] = title\n",
    "        results['values'] = value\n",
    "        marinetraffic_write_to_db(con, cur, results)\n",
    "    con.close()\n",
    "\n",
    "saving(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b5c66",
   "metadata": {},
   "source": [
    "### 3. Other Shit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d0e17",
   "metadata": {},
   "source": [
    "    cur.execute(\"\"\"CREATE VIEW [MarineTrafficVesselLocation-VIEW] AS SELECT imo, shipid, lat, lon, received, scrape_date\n",
    "    FROM [MarineTrafficVesselLocation] order by rowid desc limit 1;\"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TRIGGER [MarineTrafficVesselLocation-TRIGGER] INSTEAD OF INSERT \n",
    "    ON [MarineTrafficVesselLocation-VIEW]\n",
    "\n",
    "    WHEN new.received NOT IN (SELECT received FROM \n",
    "    [MarineTrafficVesselLocation-VIEW])\n",
    "\n",
    "    BEGIN INSERT INTO [MarineTrafficVesselLocation] values(new.imo, new.shipid, new.lat, new.lon, new.received, new.scrape_date);\n",
    "    END;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marinetraffic_write_to_db(con, cur, params):\n",
    "    con.commit()\n",
    "    query_sign = ''.join(['?' if i == len(params['columns'])-1 else '?, ' for i in range(len(params['columns']))])\n",
    "    cols = ''.join([i if i == params['columns'][-1] else i+', ' for i in params['columns']])\n",
    "\n",
    "    # del below:\n",
    "#    sqlite_insert_with_param1 = \n",
    "#    f\"\"\"insert or replace into MarineTrafficVesselLocation (imo, shipid, lat, lon, received, scrape_date) values\n",
    "#((select imo, received from MarineTrafficVesselLocation where received = \"2022-12-28 09:40\"), \"2022-12-28 09:40\", \"2023-01-01 15:52\");\"\"\"\n",
    "#    f\"\"\"INSERT INTO MarineTrafficVesselLocation ({cols}) VALUES ({query_sign});\"\"\"\n",
    "\n",
    "    #cur.execute(\"\"\"INSERT INTO [MarineTrafficVesselLocation-VIEW] values(1113333,\"shipid:4826396\", 34.54853, 34.54853, \"2022-12-28 09:40\",\n",
    "    #\"2023-01-01 19:52\");\"\"\")\n",
    "\n",
    "    data_tuple = tuple(params['values'])\n",
    "    print(data_tuple)\n",
    "    cur.execute(f\"\"\"INSERT INTO [MarineTrafficVesselLocation-VIEW] ({cols}) VALUES ({query_sign});\"\"\", data_tuple)\n",
    "    con.commit()\n",
    "\n",
    "    \n",
    "def saving(df):\n",
    "    dirname = cwd.parent / \"db\"\n",
    "    db_name = 'tmp_db4.sqlite'\n",
    "    con = sqlite3.connect(db_name)\n",
    "    cur = con.cursor()\n",
    "    create_table = (f'''CREATE TABLE IF NOT EXISTS MarineTrafficVesselLocation (imo INT, shipid TEXT, lat TEXT, lon TEXT,\n",
    "    received DATE, scrape_date DATE)''')\n",
    "    cur.execute(create_table)\n",
    "    make = False\n",
    "    # todo: set vor received instead of scrape_date \n",
    "    # change: order by scrape_date \n",
    "    if make:\n",
    "        cur.execute(\"\"\"CREATE VIEW [MarineTrafficVesselLocation-VIEW] AS SELECT DISTINCT imo, shipid, lat, lon, received, scrape_date\n",
    "        FROM [MarineTrafficVesselLocation] order by scrape_date desc;\"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TRIGGER [MarineTrafficVesselLocation-TRIGGER] INSTEAD OF INSERT \n",
    "        ON [MarineTrafficVesselLocation-VIEW]\n",
    "\n",
    "        WHEN new.scrape_date NOT IN (SELECT scrape_date FROM \n",
    "        [MarineTrafficVesselLocation-VIEW])\n",
    "\n",
    "        BEGIN INSERT INTO [MarineTrafficVesselLocation] values(new.imo, new.shipid, new.lat, new.lon, new.received, new.scrape_date);\n",
    "        END;\"\"\")\n",
    "    \n",
    "\n",
    "    title = df.columns.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        results = OrderedDict()\n",
    "        value = row.values.tolist()\n",
    "        results['columns'] = title\n",
    "        results['values'] = value\n",
    "        marinetraffic_write_to_db(con, cur, results)\n",
    "    con.close()\n",
    "\n",
    "# saving(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9205ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marinetraffic_write_to_db_V3(con, cur, params):\n",
    "    con.commit()\n",
    "    query_sign = ''.join(['?' if i == len(params['columns'])-1 else '?, ' for i in range(len(params['columns']))])\n",
    "    cols = ''.join([i if i == params['columns'][-1] else i+', ' for i in params['columns']])\n",
    "\n",
    "\n",
    "    data_tuple = tuple(params['values'])\n",
    "    print(data_tuple)\n",
    "    cur.execute(f\"\"\"INSERT INTO MarineTrafficVesselLocation (imo, shipid, lat, lon, received, scrape_date) \n",
    "    VALUES ({query_sign}) ON CONFLICT(imo, received) DO UPDATE SET imo={params['values'][0]}, shipid=\"{params['values'][1]}\", \n",
    "    lat=\"{params['values'][2]}\", lon=\"{params['values'][3]}\", received=\"{params['values'][4]}\",\n",
    "    scrape_date=\"{params['values'][5]}\";\"\"\", data_tuple)\n",
    "    #DO UPDATE SET scrape_date=excluded.scrape_date;\"\"\", data_tuple)\n",
    "    con.commit()\n",
    "\n",
    "    \n",
    "def saving(df):\n",
    "    dirname = cwd.parent / \"db\"\n",
    "    db_name = 'tmp_db8.sqlite'\n",
    "    con = sqlite3.connect(db_name)\n",
    "    cur = con.cursor()\n",
    "    create_table = (f'''CREATE TABLE IF NOT EXISTS MarineTrafficVesselLocation (imo INT UNIQUE, shipid TEXT, lat TEXT, lon TEXT,\n",
    "    received DATE, scrape_date DATE)''')\n",
    "    cur.execute(create_table)\n",
    "    make = False\n",
    "    # todo: set vor received instead of scrape_date \n",
    "    # change: order by scrape_date \n",
    "    if make:\n",
    "        cur.execute(\"\"\"CREATE VIEW [MarineTrafficVesselLocation-VIEW] AS SELECT DISTINCT imo, shipid, lat, lon, received, scrape_date\n",
    "        FROM [MarineTrafficVesselLocation] order by scrape_date desc;\"\"\")\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TRIGGER [MarineTrafficVesselLocation-TRIGGER] INSTEAD OF INSERT \n",
    "        ON [MarineTrafficVesselLocation-VIEW]\n",
    "\n",
    "        WHEN new.scrape_date NOT IN (SELECT scrape_date FROM \n",
    "        [MarineTrafficVesselLocation-VIEW])\n",
    "\n",
    "        BEGIN INSERT INTO [MarineTrafficVesselLocation] values(new.imo, new.shipid, new.lat, new.lon, new.received, new.scrape_date);\n",
    "        END;\"\"\")\n",
    "    \n",
    "\n",
    "    title = df.columns.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        results = OrderedDict()\n",
    "        value = row.values.tolist()\n",
    "        results['columns'] = title\n",
    "        results['values'] = value\n",
    "        marinetraffic_write_to_db_V3(con, cur, results)\n",
    "    con.close()\n",
    "\n",
    "# saving(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f44779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87859472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ed0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f707af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d69bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \"\"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"dfdf\")\n",
    "        # Maybe set up for a retry, or continue in a retry loop\n",
    "    except requests.exceptions.TooManyRedirects:\n",
    "        print(\"dddddddddd\")\n",
    "        # Tell the user their URL was bad and try a different one\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"dfdsafdfadfadfadf\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        input(\"change vpn V2\")\n",
    "        return pd.Series([np.nan, np.nan, eta, atd_raw, ship_cog, ship_sog, ship_type, mmsi, today])\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(name):\n",
    "    # name_list = name.split(' ')\n",
    "    # name_list = [i if len(i) >3 else None for i in name_list]\n",
    "    # print(name_list)\n",
    "    name = name.lower().replace('lng', '').replace(' al', '').replace('al ', '').replace('gas ', '').replace(' gas', '')\n",
    "    for index, row in df_m3.iterrows():    \n",
    "        #ship_name = row['Ship Name'].split(' ')\n",
    "        #print(SequenceMatcher(a=name,b=row['Ship Name']).ratio())\n",
    "        try:\n",
    "            if SequenceMatcher(a=name,b=row['Ship Name'].lower()).ratio() > 0.4:\n",
    "                print(name, row[\"Ship Name\"].lower(), SequenceMatcher(a=name,b=row['Ship Name'].lower()).ratio())\n",
    "                # return row[\"Ship Name\"]\n",
    "            #    return pd.Series([row[\"Ship Name\"], row[\"Capacity (cu.m.)\"]])\n",
    "                return pd.Series([np.nan, np.nan])\n",
    "            else:\n",
    "                pass\n",
    "        except AttributeError:\n",
    "            # print(row['Ship Name'])\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "    return pd.Series([np.nan, np.nan])\n",
    "        \n",
    "vessel[['found','Capacity']] = vessel.apply(lambda x: test(x['ship']), axis=1)\n",
    "vessel.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"accept-encoding\": \"gzip, deflate\",\n",
    "        \"user-agent\": \"Mozilla/5.0\",\n",
    "        \"x-requested-with\": \"XMLHttpRequest\"\n",
    "}\n",
    "    \n",
    "def get_ship_position(url):\n",
    "    \"\"\"source: https://stackoverflow.com/questions/68654987/scraping-data-from-marinetraffic-page\"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "data = get_ship_position(f\"https://www.marinetraffic.com/vesselDetails/latestPosition/shipid:{169195}\")\n",
    "ts = datetime.utcfromtimestamp(data[\"lastPos\"])\n",
    "print(\"Last known position: {} / {} @ {}\".format(data[\"lat\"], data[\"lon\"], ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08171e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "date_string = 'Dec 27, 2022 23:23 UTC'.upper()\n",
    "date_object = datetime.strptime(date_string, \"%b %d, %Y %H:%M UTC\")\n",
    "print(\"date_object =\", date_object)\n",
    "print(\"type of date_object =\", type(date_object))\n",
    "\n",
    "\n",
    "\n",
    "date_string = 'JAN 7, 14:00'.upper()\n",
    "date_object = datetime.strptime(date_string, \"%b %d, %H:%M\")\n",
    "print(\"date_object =\", date_object)\n",
    "print(\"type of date_object =\", type(date_object))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f74c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('source.db')\n",
    "df = pd.read_sql_query(\"SELECT imo FROM vessel_information\", conn) #= pd.read_csv('./data/lng_tankers_lat.csv', index_col=False)\n",
    "df.dropna(subset=['imo'], inplace=True)\n",
    "df.tail()\n",
    "\n",
    "user_agent = {'User-agent': '14.0.3 Safari'}\n",
    "session = requests.Session()\n",
    "shipid = []\n",
    "imo = []\n",
    "vessel = []\n",
    "mmsi = []\n",
    "t0 = time.time()\n",
    "for index, row in df.iterrows():\n",
    "    url = f\"https://www.marinetraffic.com/en/ais/details/ships/imo:{row['imo']}\"\n",
    "    r1 = session.get(url, headers=user_agent)\n",
    "    url_split = r1.url.split('/')\n",
    "    if len(url_split) == 11:\n",
    "        shipid.append(url_split[-4])\n",
    "        imo.append(row['imo'])\n",
    "        mmsi.append(url_split[-3])\n",
    "        vessel.append(url_split[-1])\n",
    "        time.sleep(0.7)\n",
    "        print(url_split)\n",
    "    \n",
    "df3 = pd.DataFrame(data={'imo': imo, 'shipid': shipid, 'mmsi': mmsi, 'vessel': vessel})\n",
    "df3.head()\n",
    "df3.to_sql('marinetraffic_vessel_information', conn, if_exists='replace', index = False)\n",
    "print(f\"Processing took: {round(time.time()-t0)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817453d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of fleet: https://www.nakilat.com/fleet-list/\n",
    "conn = sqlite3.connect('source.db')\n",
    "df_m3 = pd.read_html('http://www.aukevisser.nl/supertankers/gas-SP/id703.htm', skiprows = 0, header=0)[2]\n",
    "df_m3.to_sql('aukevisser', conn, if_exists='replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "adsf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
